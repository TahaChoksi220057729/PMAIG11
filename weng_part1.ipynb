{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Download dataset from Kaggle, fashion_mnist\n",
    "\n",
    "todo: explain why we choose to use fashion_mnist"
   ],
   "id": "c175d4d6132787b7"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:02.970086Z",
     "start_time": "2024-12-06T15:11:01.047830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "training_set = pd.read_csv('Dataset/fashion_data/fashion-mnist_train.csv')\n",
    "test_set = pd.read_csv('Dataset/fashion_data/fashion-mnist_test.csv')\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Separate into X and y sets\n",
    "One hot encode y_train and t_test"
   ],
   "id": "99bd1df81b11ee9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:02.977356Z",
     "start_time": "2024-12-06T15:11:02.974608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # One hot encode\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)"
   ],
   "id": "d5f6b4a07a57bad8",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:03.172989Z",
     "start_time": "2024-12-06T15:11:02.991873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# X is pixels, y is labels\n",
    "\n",
    "X_train, y_train = to_xy(training_set, 'label')\n",
    "X_test, y_test = to_xy(test_set, 'label')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test[0])"
   ],
   "id": "e0306a4ea9e7d442",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:03.226790Z",
     "start_time": "2024-12-06T15:11:03.186112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#visualising one image\n",
    "\n",
    "# choosing the first image\n",
    "image_array = X_train[0]\n",
    "label = y_train[0]\n",
    "\n",
    "# reshape 784 to 28x28\n",
    "image_array = image_array.reshape(28, 28)\n",
    "\n",
    "# plotting the image\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "c3ad543f1f410c0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:03.280308Z",
     "start_time": "2024-12-06T15:11:03.238956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# rescaling the pixel values down to values between 0 - 1 for efficiency of neural network\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ],
   "id": "e19cba663018fe9a",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:03.299477Z",
     "start_time": "2024-12-06T15:11:03.292697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# activation functions and its derivatives\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.x_copy = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #dout in: (60000, 15)\n",
    "        #x_copy: (60000, 15)\n",
    "        dout = dout * (self.x_copy > 0)\n",
    "        #dout out: (60000, 15)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.x_copy = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return self.sigmoid(self.x_copy) * (1 - self.sigmoid(self.x_copy)) * dout\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self, crossEntropy=False):\n",
    "        self.x_copy = None\n",
    "        self.cross_entropy = crossEntropy\n",
    "\n",
    "    def softmax(self, x):\n",
    "        stable_exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        y_prob = stable_exp_x / np.sum(stable_exp_x, axis=1, keepdims=True)\n",
    "        return y_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if self.cross_entropy:\n",
    "            return dout\n",
    "        else:\n",
    "            softmax_output = self.softmax(self.x_copy)\n",
    "            return softmax_output * (dout - (dout * softmax_output).sum(axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_num, output_num):\n",
    "        self.db = None\n",
    "        self.dw = None\n",
    "        self.rng = np.random.default_rng(42)\n",
    "        # initialise weights with Xavier initialisation\n",
    "        self.weights = self.rng.normal(size=(input_num, output_num)) * np.sqrt(2 / input_num)\n",
    "        self.bias = np.zeros((1, output_num))\n",
    "        self.X_copy = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X_copy = X\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Gradient with respect to weights\n",
    "        dw = np.dot(self.X_copy.T, dout)  # Shape: (input_num, output_num)\n",
    "        \n",
    "        # Gradient with respect to biases\n",
    "        db = np.sum(dout, axis=0, keepdims=True)  # Shape: (1, output_num)\n",
    "\n",
    "        # Gradient with respect to inputs\n",
    "        dx = np.dot(dout, self.weights.T)  # Shape: (batch_size, input_num)\n",
    "\n",
    "        # Save gradients for weight and bias updates\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        if self.dw is not None and self.db is not None:\n",
    "            self.weights -= learning_rate * self.dw\n",
    "            self.bias -= learning_rate * self.db\n",
    "        else:\n",
    "            print(\"Error: gradients not detected.\")\n",
    "\n",
    "\n",
    "class CrossEntropy:\n",
    "    def __init__(self, softmax=False):\n",
    "        self.y_pred_copy = None\n",
    "        self.softmax = softmax\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        self.y_pred_copy = np.clip(y_pred, 1e-8, 1)\n",
    "        loss = -np.mean(np.sum(y * np.log(self.y_pred_copy), axis=1)) # axis = 1 to sum across classes\n",
    "        return loss \n",
    "\n",
    "    def backward(self, y):\n",
    "        if self.softmax:\n",
    "            return self.y_pred_copy - y\n",
    "        # -y / y_pred / y.shape[0]\n",
    "        return np.where(y == 1, - y / (self.y_pred_copy + 1e-15) / y.shape[0], 0)"
   ],
   "id": "2246ea3f1eb50fb8",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:03.316203Z",
     "start_time": "2024-12-06T15:11:03.312393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss_fn, learning_rate):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        self.learning_rate = learning_rate\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return self.loss_fn.forward(y, y_pred)\n",
    "\n",
    "    def backward(self, y_train):\n",
    "        grad = self.loss_fn.backward(y_train)\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            grad = self.layers[i].backward(grad)\n",
    "\n",
    "    def update(self):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.update_weights(self.learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        correct_predictions = np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1)\n",
    "        accuracy = np.mean(correct_predictions)\n",
    "        return accuracy\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100):\n",
    "        loss_hist = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X_train)\n",
    "            loss = self.loss(y_train, y_pred)\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "            loss_hist.append(loss)\n",
    "            self.backward(y_train)\n",
    "            self.update()\n",
    "        return loss_hist"
   ],
   "id": "9a078cf45c4abbdb",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:03.330806Z",
     "start_time": "2024-12-06T15:11:03.329031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_loss(loss_hist):\n",
    "    plt.plot(loss_hist)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "3dbf0b8ed880df79",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:11.001892Z",
     "start_time": "2024-12-06T15:11:03.342911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = NeuralNetwork(\n",
    "    [\n",
    "        Linear(X_train.shape[1], 15),\n",
    "        ReLU(),\n",
    "        Linear(15, 10),\n",
    "        Softmax(crossEntropy=True)\n",
    "    ],\n",
    "    CrossEntropy(softmax=True), 0.01)\n",
    "\n",
    "loss_hist = model.train(X_train, y_train, epochs=100)\n",
    "plot_loss(loss_hist)"
   ],
   "id": "cd4faca7dc18eb51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.4786546313924673\n",
      "Epoch: 10, Loss: 16.57861266955713\n",
      "Epoch: 20, Loss: 16.57861266955713\n",
      "Epoch: 30, Loss: 16.57861266955713\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[101], line 10\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m NeuralNetwork(\n\u001B[1;32m      2\u001B[0m     [\n\u001B[1;32m      3\u001B[0m         Linear(X_train\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;241m15\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m     ],\n\u001B[1;32m      8\u001B[0m     CrossEntropy(softmax\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), \u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m loss_hist \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m plot_loss(loss_hist)\n",
      "Cell \u001B[0;32mIn[99], line 43\u001B[0m, in \u001B[0;36mNeuralNetwork.train\u001B[0;34m(self, X_train, y_train, epochs)\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     42\u001B[0m     loss_hist\u001B[38;5;241m.\u001B[39mappend(loss)\n\u001B[0;32m---> 43\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss_hist\n",
      "Cell \u001B[0;32mIn[99], line 19\u001B[0m, in \u001B[0;36mNeuralNetwork.backward\u001B[0;34m(self, y_train)\u001B[0m\n\u001B[1;32m     17\u001B[0m grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn\u001B[38;5;241m.\u001B[39mbackward(y_train)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 19\u001B[0m     grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[98], line 72\u001B[0m, in \u001B[0;36mLinear.backward\u001B[0;34m(self, dout)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackward\u001B[39m(\u001B[38;5;28mself\u001B[39m, dout):\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;66;03m# Gradient with respect to weights\u001B[39;00m\n\u001B[0;32m---> 72\u001B[0m     dw \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mX_copy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdout\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Shape: (input_num, output_num)\u001B[39;00m\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;66;03m# Gradient with respect to biases\u001B[39;00m\n\u001B[1;32m     75\u001B[0m     db \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(dout, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)  \u001B[38;5;66;03m# Shape: (1, output_num)\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:11.062850Z",
     "start_time": "2024-12-06T15:10:24.435935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "acc = model.evaluate(X_test, y_test)\n",
    "print(acc)"
   ],
   "id": "ef2a8599ae3473dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T15:11:11.062962Z",
     "start_time": "2024-12-06T15:10:24.464731Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5c7636553c68faea",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
