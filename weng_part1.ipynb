{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Download dataset from Kaggle, fashion_mnist\n",
    "\n",
    "todo: explain why we choose to use fashion_mnist"
   ],
   "id": "c175d4d6132787b7"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "training_set = pd.read_csv('Dataset/fashion_data/fashion-mnist_train.csv')\n",
    "test_set = pd.read_csv('Dataset/fashion_data/fashion-mnist_test.csv')\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Separate into X and y sets\n",
    "One hot encode y_train and t_test"
   ],
   "id": "99bd1df81b11ee9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # One hot encode\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)"
   ],
   "id": "d5f6b4a07a57bad8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# X is pixels, y is labels\n",
    "\n",
    "X_train, y_train = to_xy(training_set, 'label')\n",
    "X_test, y_test = to_xy(test_set, 'label')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test[0])"
   ],
   "id": "e0306a4ea9e7d442",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#visualising one image\n",
    "\n",
    "# choosing the first image\n",
    "image_array = X_train[0]\n",
    "label = y_train[0]\n",
    "\n",
    "# reshape 784 to 28x28\n",
    "image_array = image_array.reshape(28, 28)\n",
    "\n",
    "# plotting the image\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "c3ad543f1f410c0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# rescaling the pixel values down to values between 0 - 1 for efficiency of neural network\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# X_train = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "# X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)"
   ],
   "id": "e19cba663018fe9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# activation functions and its derivatives\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.x_copy = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        out = np.maximum(0, x)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #dout in: (60000, 15)\n",
    "        #x_copy: (60000, 15)\n",
    "        dout = dout * (self.x_copy > 0)\n",
    "        #dout out: (60000, 15)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.x_copy = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return self.sigmoid(self.x_copy) * (1 - self.sigmoid(self.x_copy)) * dout\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self, crossEntropy=False):\n",
    "        self.x_copy = None\n",
    "        self.cross_entropy = crossEntropy\n",
    "\n",
    "    def softmax(self, x):\n",
    "        stable_exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        y_prob = stable_exp_x / np.sum(stable_exp_x, axis=1, keepdims=True)\n",
    "        return y_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        out = self.softmax(x)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if self.cross_entropy:\n",
    "            return dout\n",
    "        else:\n",
    "            softmax_output = self.softmax(self.x_copy)\n",
    "            return softmax_output * (dout - (dout * softmax_output).sum(axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_num, output_num, weight_init=\"he\"):\n",
    "        self.db = None\n",
    "        self.dw = None\n",
    "        self.rng = np.random.default_rng(42)\n",
    "\n",
    "        if weight_init == \"he\":\n",
    "            self.weights = self.rng.normal(size=(input_num, output_num)) * np.sqrt(2 / input_num)\n",
    "        elif weight_init == \"xavier\":\n",
    "            self.weights = self.rng.normal(size=(input_num, output_num)) * np.sqrt(1 / input_num)\n",
    "        self.bias = np.zeros((1, output_num))\n",
    "        self.X_copy = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X_copy = X\n",
    "        out = np.dot(X, self.weights) + self.bias\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Gradient with respect to weights\n",
    "        dw = np.dot(self.X_copy.T, dout)  # Shape: (input_num, output_num)\n",
    "\n",
    "        # Gradient with respect to biases\n",
    "        db = np.sum(dout, axis=0, keepdims=True)  # Shape: (1, output_num)\n",
    "\n",
    "        # Gradient with respect to inputs\n",
    "        dx = np.dot(dout, self.weights.T)  # Shape: (batch_size, input_num)\n",
    "\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self, learning_rate, l2_reg=1e-4):\n",
    "        assert self.dw is not None and self.db is not None, \"Gradients must be computed before updating weights\"\n",
    "        self.weights -= learning_rate * (self.dw + l2_reg * self.weights)\n",
    "        self.bias -= learning_rate * self.db\n",
    "\n",
    "\n",
    "class CrossEntropy:\n",
    "    def __init__(self, softmax=False):\n",
    "        self.y_pred_copy = None\n",
    "        self.softmax = softmax\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        self.y_pred_copy = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "        loss = -np.mean(np.sum(y * np.log(self.y_pred_copy), axis=1)) # axis = 1 to sum across classes\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y):\n",
    "        if self.softmax:\n",
    "            return self.y_pred_copy - y\n",
    "        # -y / y_pred / y.shape[0]\n",
    "        return np.where(y == 1, - y / (self.y_pred_copy + 1e-15) / y.shape[0], 0)"
   ],
   "id": "2246ea3f1eb50fb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def decaying_lr(epoch, initial_lr=1e-2, decay_rate = 0.1):\n",
    "    lr = np.clip(initial_lr / (1 + decay_rate * epoch), 1e-4, 1e-2)\n",
    "    return lr"
   ],
   "id": "377a42dbebb1c227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dropout Layer",
   "id": "a3cd3d2423f3c2fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class InvertedDropout:\n",
    "    def __init__(self, rate):\n",
    "        self.dropout_rate = rate\n",
    "        self.mask = None\n",
    "        self.rng = np.random.default_rng(42)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            init_mask = self.rng.random(x.shape)\n",
    "            self.mask = (init_mask > self.dropout_rate).astype(np.float32)\n",
    "            return x * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ],
   "id": "cca2379740b887d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss_fn):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        return\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Inverted_Dropout):\n",
    "              x = layer.forward(x, training=training)\n",
    "            else:\n",
    "              x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return self.loss_fn.forward(y, y_pred)\n",
    "\n",
    "    def backward(self, y_train):\n",
    "        grad = self.loss_fn.backward(y_train)\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            grad = np.clip(grad, -2, 2)\n",
    "            grad = self.layers[i].backward(grad)\n",
    "\n",
    "    def update(self, epoch):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.update_weights(decaying_lr(epoch))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X, training=False)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        correct_predictions = np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1)\n",
    "        accuracy = np.mean(correct_predictions)\n",
    "        return accuracy\n",
    "\n",
    "    def batch_train(self, X_train, y_train, epochs=100):\n",
    "        loss_hist = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X_train)\n",
    "            loss = self.loss(y_train, y_pred)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "            loss_hist.append(loss)\n",
    "            self.backward(y_train)\n",
    "            self.update(epoch)\n",
    "        return loss_hist\n",
    "\n",
    "    def mini_batch_train(self, X_train, y_train, epochs=100, batch=128):\n",
    "      loss_hist = []\n",
    "      avg_loss_hist = []\n",
    "      for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train), batch):\n",
    "          X_train_batch = X_train[i: i+batch]\n",
    "          y_train_batch = y_train[i: i+batch]\n",
    "          y_pred = self.forward(X_train_batch)\n",
    "          loss = self.loss(y_train_batch, y_pred)\n",
    "          loss_hist.append(loss)\n",
    "          self.backward(y_train_batch)\n",
    "          self.update(epoch)\n",
    "        avg_loss = np.mean(loss_hist)\n",
    "        avg_loss_hist.append(avg_loss)\n",
    "        if epoch % 5 == 0:\n",
    "              print(f\"Epoch: {epoch}, Training Loss: {avg_loss}\")\n",
    "        loss_hist.clear()\n",
    "      return avg_loss_hist"
   ],
   "id": "9a078cf45c4abbdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_loss(loss_hist):\n",
    "    plt.plot(loss_hist)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ],
   "id": "3dbf0b8ed880df79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initialising a model",
   "id": "a6a8a13bc8316e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = NeuralNetwork(\n",
    "    [\n",
    "        Linear(X_train.shape[1], 600),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.3),\n",
    "        Linear(600, 300),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.3),\n",
    "        Linear(300,100),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.3),\n",
    "        Linear(100,50),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.3),\n",
    "        Linear(50, 10,weight_init=\"xavier\"),\n",
    "        Softmax(crossEntropy=True)\n",
    "    ],\n",
    "    CrossEntropy(softmax=True))"
   ],
   "id": "cd4faca7dc18eb51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training model & plotting loss",
   "id": "d25a65dadd8579f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_sample = X_train\n",
    "y_train_sample = y_train\n",
    "loss_hist = model.train(X_train_sample, y_train_sample, epochs=100)\n",
    "plot_loss(loss_hist)"
   ],
   "id": "27240361d33a2686",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate model",
   "id": "9c7d429774e69b3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "acc = model.evaluate(X_test, y_test)\n",
    "print(acc)"
   ],
   "id": "ef2a8599ae3473dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_pred = model.predict(X_test[:5])\n",
    "y = y_test[:5]\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "true_classes = np.argmax(y_test[:5], axis=1)\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "print(\"True classes:\", true_classes)"
   ],
   "id": "5c7636553c68faea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4619fb5d9e8f5834",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
