{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c175d4d6132787b7",
   "metadata": {},
   "source": [
    "### Download dataset from Kaggle, fashion_mnist\n",
    "\n",
    "todo: explain why we choose to use fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.037063Z",
     "start_time": "2024-12-13T16:43:39.648709Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set = pd.read_csv('Dataset/fashion_data/fashion-mnist_train.csv')\n",
    "test_set = pd.read_csv('Dataset/fashion_data/fashion-mnist_test.csv')\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd1df81b11ee9",
   "metadata": {},
   "source": [
    "### Separate into X and y sets\n",
    "One hot encode y_train and t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f6b4a07a57bad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.096332Z",
     "start_time": "2024-12-13T16:43:47.092654Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_xy(df, target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "    # find out the type of the target column\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(\n",
    "        target_type, '__iter__') else target_type\n",
    "    # Encode to int for classification, float otherwise. TensorFlow likes 32 bits.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # One hot encode\n",
    "        dummies = pd.get_dummies(df[target])\n",
    "        return df[result].values.astype(np.float32), dummies.values.astype(np.float32)\n",
    "    # Regression\n",
    "    return df[result].values.astype(np.float32), df[[target]].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0306a4ea9e7d442",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.434385Z",
     "start_time": "2024-12-13T16:43:47.267465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# X is pixels, y is labels\n",
    "\n",
    "X_train, y_train = to_xy(training_set, 'label')\n",
    "X_test, y_test = to_xy(test_set, 'label')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d51287bfca754f",
   "metadata": {},
   "source": [
    "creating validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b680cc04651ac390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.722246Z",
     "start_time": "2024-12-13T16:43:47.456336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 784)\n",
      "(54000, 10)\n",
      "(6000, 784)\n",
      "(6000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3ad543f1f410c0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.838572Z",
     "start_time": "2024-12-13T16:43:47.744225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualising one image\n",
    "\n",
    "# choosing the first image\n",
    "image_array = X_train[0]\n",
    "label = y_train[0]\n",
    "\n",
    "# reshape 784 to 28x28\n",
    "image_array = image_array.reshape(28, 28)\n",
    "\n",
    "# plotting the image\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e19cba663018fe9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.895809Z",
     "start_time": "2024-12-13T16:43:47.853499Z"
    }
   },
   "outputs": [],
   "source": [
    "# rescaling the pixel values down to values between 0 - 1 for efficiency of neural network\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2246ea3f1eb50fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.917202Z",
     "start_time": "2024-12-13T16:43:47.909609Z"
    }
   },
   "outputs": [],
   "source": [
    "# activation functions and its derivatives\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.x_copy = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        out = np.maximum(0, x)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #dout in: (60000, 15)\n",
    "        #x_copy: (60000, 15)\n",
    "        dout = dout * (self.x_copy > 0)\n",
    "        #dout out: (60000, 15)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.x_copy = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return self.sigmoid(self.x_copy) * (1 - self.sigmoid(self.x_copy)) * dout\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self, crossEntropy=False):\n",
    "        self.x_copy = None\n",
    "        self.cross_entropy = crossEntropy\n",
    "\n",
    "    def softmax(self, x):\n",
    "        stable_exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        y_prob = stable_exp_x / np.sum(stable_exp_x, axis=1, keepdims=True)\n",
    "        return y_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        out = self.softmax(x)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if self.cross_entropy:\n",
    "            return dout\n",
    "        else:\n",
    "            softmax_output = self.softmax(self.x_copy)\n",
    "            return softmax_output * (dout - (dout * softmax_output).sum(axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, input_num, output_num, weight_init=\"he\"):\n",
    "        self.db = None\n",
    "        self.dw = None\n",
    "        self.rng = np.random.default_rng(42)\n",
    "\n",
    "        if weight_init == \"he\":\n",
    "            self.weights = self.rng.normal(size=(input_num, output_num)) * np.sqrt(2 / input_num)\n",
    "        elif weight_init == \"xavier\":\n",
    "            self.weights = self.rng.normal(size=(input_num, output_num)) * np.sqrt(1 / input_num)\n",
    "        self.bias = np.zeros((1, output_num))\n",
    "        self.X_copy = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X_copy = X\n",
    "        out = np.dot(X, self.weights) + self.bias\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Gradient with respect to weights\n",
    "        dw = np.dot(self.X_copy.T, dout)  # Shape: (input_num, output_num)\n",
    "\n",
    "        # Gradient with respect to biases\n",
    "        db = np.sum(dout, axis=0, keepdims=True)  # Shape: (1, output_num)\n",
    "\n",
    "        # Gradient with respect to inputs\n",
    "        dx = np.dot(dout, self.weights.T)  # Shape: (batch_size, input_num)\n",
    "\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self, learning_rate, l2_reg=1e-4):\n",
    "        assert self.dw is not None and self.db is not None, \"Gradients must be computed before updating weights\"\n",
    "        self.weights -= learning_rate * (self.dw + l2_reg * self.weights)\n",
    "        self.bias -= learning_rate * self.db\n",
    "\n",
    "\n",
    "class CrossEntropy:\n",
    "    def __init__(self, softmax=False):\n",
    "        self.y_pred_copy = None\n",
    "        self.softmax = softmax\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        self.y_pred_copy = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "        loss = -np.mean(np.sum(y * np.log(self.y_pred_copy), axis=1)) # axis = 1 to sum across classes\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y):\n",
    "        if self.softmax:\n",
    "            return self.y_pred_copy - y\n",
    "        # -y / y_pred / y.shape[0]\n",
    "        return np.where(y == 1, - y / (self.y_pred_copy + 1e-15) / y.shape[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "377a42dbebb1c227",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.933320Z",
     "start_time": "2024-12-13T16:43:47.931186Z"
    }
   },
   "outputs": [],
   "source": [
    "def decaying_lr(epoch, initial_lr=1e-2, decay_rate = 0.1):\n",
    "    lr = np.clip(initial_lr / (1 + decay_rate * epoch), 1e-4, 1e-2)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd3d2423f3c2fa",
   "metadata": {},
   "source": [
    "Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca2379740b887d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.954563Z",
     "start_time": "2024-12-13T16:43:47.951705Z"
    }
   },
   "outputs": [],
   "source": [
    "class InvertedDropout:\n",
    "    def __init__(self, rate):\n",
    "        self.dropout_rate = rate\n",
    "        self.mask = None\n",
    "        self.rng = np.random.default_rng(42)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            init_mask = self.rng.random(x.shape)\n",
    "            self.mask = (init_mask > self.dropout_rate).astype(np.float32)\n",
    "            return x * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f07cc194a87f01",
   "metadata": {},
   "source": [
    "Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29174688403e7f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.970377Z",
     "start_time": "2024-12-13T16:43:47.967705Z"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  def __init__(self, patience=5, min_change=0.01):\n",
    "    self.patience = patience\n",
    "    self.min_change = min_change\n",
    "    self.prev_loss = None\n",
    "    self.penalty_count = 0\n",
    "\n",
    "  def track(self, val_loss):\n",
    "    if self.prev_loss is None:\n",
    "      self.prev_loss = val_loss\n",
    "      return True\n",
    "    if self.prev_loss - val_loss <= self.min_change:\n",
    "      self.penalty_count += 1\n",
    "    else:\n",
    "      self.penalty_count = 0\n",
    "    self.prev_loss = val_loss\n",
    "    return self.penalty_count <= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a078cf45c4abbdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:47.990331Z",
     "start_time": "2024-12-13T16:43:47.983487Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss_fn):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        return\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, InvertedDropout):\n",
    "              x = layer.forward(x, training=training)\n",
    "            else:\n",
    "              x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return self.loss_fn.forward(y, y_pred)\n",
    "\n",
    "    def backward(self, y_train):\n",
    "        grad = self.loss_fn.backward(y_train)\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            grad = np.clip(grad, -2, 2)\n",
    "            grad = self.layers[i].backward(grad)\n",
    "\n",
    "    def update(self, epoch):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.update_weights(decaying_lr(epoch))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X, training=False)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        correct_predictions = np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1)\n",
    "        accuracy = np.mean(correct_predictions)\n",
    "        return accuracy\n",
    "\n",
    "    def batch_train(self, X_train, y_train, epochs=100):\n",
    "        loss_hist = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X_train)\n",
    "            loss = self.loss(y_train, y_pred)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "            loss_hist.append(loss)\n",
    "            self.backward(y_train)\n",
    "            self.update(epoch)\n",
    "        return loss_hist\n",
    "\n",
    "    def mini_batch_train(self, X_train, y_train, X_val, y_val, earlyStopping=False, epochs=100, batch=128):\n",
    "      loss_hist = []\n",
    "      avg_loss_hist = []\n",
    "      val_loss_hist = []\n",
    "      if earlyStopping:\n",
    "        cb = EarlyStopping()\n",
    "      for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train), batch):\n",
    "          X_train_batch = X_train[i: i+batch]\n",
    "          y_train_batch = y_train[i: i+batch]\n",
    "          y_pred = self.forward(X_train_batch)\n",
    "          loss = self.loss(y_train_batch, y_pred)\n",
    "          loss_hist.append(loss)\n",
    "          self.backward(y_train_batch)\n",
    "          self.update(epoch)\n",
    "        avg_loss = np.mean(loss_hist)\n",
    "        avg_loss_hist.append(avg_loss)\n",
    "        val_loss = self.predict_val(X_val, y_val)\n",
    "        val_loss_hist.append(val_loss)\n",
    "        if epoch % 5 == 0:\n",
    "              print(f\"Epoch: {epoch}, training loss: {avg_loss}, val loss: {val_loss}\")\n",
    "        criterea = cb.track(val_loss)\n",
    "        if not criterea:\n",
    "          print(f\"Early stopping on epoch {epoch}\")\n",
    "          return avg_loss_hist, val_loss_hist\n",
    "        loss_hist.clear()\n",
    "      return avg_loss_hist, val_loss_hist\n",
    "\n",
    "    def predict_val(self, X_val, y_val):\n",
    "      y_pred = self.forward(X_val)\n",
    "      loss = self.loss(y_val, y_pred)\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dbf0b8ed880df79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:48.012930Z",
     "start_time": "2024-12-13T16:43:48.010799Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(loss_hist, val_loss):\n",
    "    plt.plot(loss_hist, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss', linestyle='--')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Val Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8a13bc8316e4",
   "metadata": {},
   "source": [
    "### Initialising a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd4faca7dc18eb51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:43:48.033505Z",
     "start_time": "2024-12-13T16:43:48.025806Z"
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    [\n",
    "        Linear(X_train.shape[1], 600),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.5),\n",
    "        Linear(600, 300),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.5),\n",
    "        Linear(300,100),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.3),\n",
    "        # Linear(100,50),\n",
    "        # ReLU(),\n",
    "        InvertedDropout(0.3),\n",
    "        Linear(100, 10,weight_init=\"xavier\"),\n",
    "        Softmax(crossEntropy=True)\n",
    "    ],\n",
    "    CrossEntropy(softmax=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25a65dadd8579f3",
   "metadata": {},
   "source": [
    "Training model & plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27240361d33a2686",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:52:56.730804Z",
     "start_time": "2024-12-13T16:43:48.046620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, training loss: 1.0066237202034047, val loss: 10.340173576859033\n",
      "Epoch: 5, training loss: 0.4597730226842099, val loss: 7.272696175673612\n",
      "Epoch: 10, training loss: 0.40192380389997273, val loss: 6.451985695363609\n",
      "Epoch: 15, training loss: 0.3673819023501282, val loss: 6.20366691913284\n",
      "Epoch: 20, training loss: 0.3401529637883814, val loss: 6.029765460321245\n",
      "Epoch: 25, training loss: 0.3261489664230248, val loss: 5.375810763697796\n",
      "Epoch: 30, training loss: 0.3115091897241905, val loss: 6.302075116160108\n",
      "Epoch: 35, training loss: 0.3014369983884754, val loss: 6.044237518776538\n",
      "Epoch: 40, training loss: 0.290120890608026, val loss: 5.862320507865927\n",
      "Epoch: 45, training loss: 0.2826438647345252, val loss: 5.856828841062569\n",
      "Epoch: 50, training loss: 0.27613780086447715, val loss: 6.171282863856834\n",
      "Epoch: 55, training loss: 0.26957851658076726, val loss: 6.092787725835562\n",
      "Epoch: 60, training loss: 0.26202336200417753, val loss: 6.036788168031792\n",
      "Epoch: 65, training loss: 0.2582527460308308, val loss: 5.933114986240864\n",
      "Epoch: 70, training loss: 0.25294260667949553, val loss: 6.2064726522987135\n",
      "Epoch: 75, training loss: 0.24828600793688907, val loss: 6.138342726415178\n",
      "Epoch: 80, training loss: 0.2459131819641093, val loss: 5.9498997736571955\n",
      "Epoch: 85, training loss: 0.23753001350867364, val loss: 5.833455170996128\n",
      "Epoch: 90, training loss: 0.23749488301561375, val loss: 6.130256055818151\n",
      "Epoch: 95, training loss: 0.2349208379930379, val loss: 6.104865405539847\n",
      "Epoch: 100, training loss: 0.22941643920159485, val loss: 5.832563241729574\n",
      "Epoch: 105, training loss: 0.22749785080552445, val loss: 6.018205948852138\n",
      "Epoch: 110, training loss: 0.22779763174017426, val loss: 6.175104123505294\n",
      "Epoch: 115, training loss: 0.22161227234428635, val loss: 5.867667445061019\n",
      "Epoch: 120, training loss: 0.21758540446768934, val loss: 6.118977324046003\n",
      "Epoch: 125, training loss: 0.21166779488335696, val loss: 6.232557689506744\n",
      "Epoch: 130, training loss: 0.21379956100103967, val loss: 6.075889542037562\n",
      "Epoch: 135, training loss: 0.21065799933470972, val loss: 6.217990029600525\n",
      "Epoch: 140, training loss: 0.2092994907920275, val loss: 6.2355635768582385\n",
      "Epoch: 145, training loss: 0.20566955100649112, val loss: 6.100277815275349\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plot_loss() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m y_train_sample \u001B[38;5;241m=\u001B[39m y_train\n\u001B[1;32m      3\u001B[0m loss_hist, val_loss_hist \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mmini_batch_train(X_train_sample, y_train_sample, X_val, y_val, earlyStopping\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m150\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m \u001B[43mplot_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss_hist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loss_hist\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: plot_loss() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "X_train_sample = X_train\n",
    "y_train_sample = y_train\n",
    "loss_hist, val_loss_hist = model.mini_batch_train(X_train_sample, y_train_sample, X_val, y_val, earlyStopping=True, epochs=150)\n",
    "plot_loss(loss_hist, val_loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d429774e69b3a",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a8599ae3473dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.evaluate(X_test, y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7636553c68faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test[:5])\n",
    "y = y_test[:5]\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "true_classes = np.argmax(y_test[:5], axis=1)\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "print(\"True classes:\", true_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4619fb5d9e8f5834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
