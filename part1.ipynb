{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_set = pd.read_csv('Dataset/fashion-mnist_train.csv')\n",
    "test_set = pd.read_csv('Dataset/fashion-mnist_test.csv')\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df, target):\n",
    "    \n",
    "    # Extract feature columns\n",
    "    feature_columns = [col for col in df.columns if col != target]\n",
    "    \n",
    "    # Determine the type of the target column\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Convert features to float32\n",
    "    X = df[feature_columns].values.astype(np.float32)\n",
    "    \n",
    "    # Convert target to float32\n",
    "    y = df[target].values.astype(np.float32)\n",
    "    \n",
    "    # Handle target based on type\n",
    "    if target_type in (np.int64, np.int32):\n",
    "    # One hot encode for classification\n",
    "        y = pd.get_dummies(df[target]).values.astype(np.float32)\n",
    "    else:\n",
    "    # For regression, just convert to float32\n",
    "        y = df[target].values.astype(np.float32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# X is pixels, y is labels\n",
    "\n",
    "X_train, y_train = data_prep(training_set, 'label')\n",
    "X_test, y_test = data_prep(test_set, 'label')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 784)\n",
      "(54000, 10)\n",
      "(6000, 784)\n",
      "(6000, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# choosing an image\n",
    "image_array = X_train[69]\n",
    "label = y_train[69]\n",
    "\n",
    "# reshape 784 to 28x28\n",
    "image_array = image_array.reshape(28, 28)\n",
    "\n",
    "# plotting the image\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling the pixel values down to values between 0 - 1 for efficiency of neural network\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.x_copy = None\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return self.sigmoid(self.x_copy) * (1 - self.sigmoid(self.x_copy)) * dout\n",
    "    \n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.x_copy = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        out = np.maximum(0, x)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        #dout in: (60000, 15)\n",
    "        #x_copy: (60000, 15)\n",
    "        dout = dout * (self.x_copy > 0)\n",
    "        #dout out: (60000, 15)\n",
    "        return dout\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.name = 'leaky_relu'\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1, self.alpha)\n",
    "\n",
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.name = 'tanh'\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return 1 - np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, crossEntropy=False):\n",
    "        self.x_copy = None\n",
    "        self.cross_entropy = crossEntropy\n",
    "\n",
    "    def softmax(self, x):\n",
    "        stable_exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        y_prob = stable_exp_x / np.sum(stable_exp_x, axis=1, keepdims=True)\n",
    "        return y_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        out = self.softmax(x)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if self.cross_entropy:\n",
    "            return dout\n",
    "        else:\n",
    "            softmax_output = self.softmax(self.x_copy)\n",
    "            return softmax_output * (dout - (dout * softmax_output).sum(axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, input_num, output_num, weight_init=\"he\"):\n",
    "        self.db = None\n",
    "        self.dw = None\n",
    "        self.rng = np.random.default_rng(42)\n",
    "\n",
    "        if weight_init == \"he\":\n",
    "            self.weights = self.rng.normal(size=(input_num, output_num)) * np.sqrt(2 / input_num)\n",
    "        elif weight_init == \"xavier\":\n",
    "            self.weights = self.rng.normal(size=(input_num, output_num)) * np.sqrt(1 / input_num)\n",
    "        self.bias = np.zeros((1, output_num))\n",
    "        self.X_copy = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X_copy = X\n",
    "        out = np.dot(X, self.weights) + self.bias\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Gradient with respect to weights\n",
    "        dw = np.dot(self.X_copy.T, dout)  # Shape: (input_num, output_num)\n",
    "\n",
    "        # Gradient with respect to biases\n",
    "        db = np.sum(dout, axis=0, keepdims=True)  # Shape: (1, output_num)\n",
    "\n",
    "        # Gradient with respect to inputs\n",
    "        dx = np.dot(dout, self.weights.T)  # Shape: (batch_size, input_num)\n",
    "\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update_weights(self, learning_rate, l2_reg=1e-4):\n",
    "        assert self.dw is not None and self.db is not None, \"Gradients must be computed before updating weights\"\n",
    "        self.weights -= learning_rate * (self.dw + l2_reg * self.weights)\n",
    "        self.bias -= learning_rate * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    def __init__(self, softmax=False):\n",
    "        self.y_pred_copy = None\n",
    "        self.softmax = softmax\n",
    "\n",
    "    def forward(self, y, y_pred):\n",
    "        self.y_pred_copy = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "        loss = -np.mean(np.sum(y * np.log(self.y_pred_copy), axis=1)) # axis = 1 to sum across classes\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y):\n",
    "        if self.softmax:\n",
    "            return self.y_pred_copy - y\n",
    "        # -y / y_pred / y.shape[0]\n",
    "        return np.where(y == 1, - y / (self.y_pred_copy + 1e-15) / y.shape[0], 0)\n",
    "    \n",
    "def decaying_lr(epoch, initial_lr=1e-2, decay_rate = 0.1):\n",
    "    lr = np.clip(initial_lr / (1 + decay_rate * epoch), 1e-4, 1e-2)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedDropout:\n",
    "    def __init__(self, rate):\n",
    "        self.dropout_rate = rate\n",
    "        self.mask = None\n",
    "        self.rng = np.random.default_rng(42)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            init_mask = self.rng.random(x.shape)\n",
    "            self.mask = (init_mask > self.dropout_rate).astype(np.float32)\n",
    "            return x * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  def __init__(self, patience=5, min_change=0.01):\n",
    "    self.patience = patience\n",
    "    self.min_change = min_change\n",
    "    self.prev_loss = None\n",
    "    self.penalty_count = 0\n",
    "\n",
    "  def track(self, val_loss):\n",
    "    if self.prev_loss is None:\n",
    "      self.prev_loss = val_loss\n",
    "      return True\n",
    "    if self.prev_loss - val_loss <= self.min_change:\n",
    "      self.penalty_count += 1\n",
    "    else:\n",
    "      self.penalty_count = 0\n",
    "    self.prev_loss = val_loss\n",
    "    return self.penalty_count <= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.num_features = num_features\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "        self.running_mean = np.zeros((1, num_features))\n",
    "        self.running_var = np.ones((1, num_features))\n",
    "        self.training = True\n",
    "        # Store input for backward pass\n",
    "        self.input = None\n",
    "\n",
    "        self.gamma_momentum = np.zeros_like(self.gamma)\n",
    "        self.beta_momentum = np.zeros_like(self.beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store input for backward pass\n",
    "        self.input = x\n",
    "        \n",
    "        if self.training:\n",
    "            batch_mean = np.mean(x, axis=0, keepdims=True)\n",
    "            batch_var = np.var(x, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "            \n",
    "            # Normalize\n",
    "            x_norm = (x - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            \n",
    "            # Store values for backward pass\n",
    "            self.batch_mean = batch_mean\n",
    "            self.batch_var = batch_var\n",
    "            self.x_norm = x_norm\n",
    "            \n",
    "        else:\n",
    "            # Use running statistics for inference\n",
    "            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Get batch size\n",
    "        m = dout.shape[0]\n",
    "        \n",
    "        # Gradients for beta and gamma\n",
    "        self.dbeta = np.sum(dout, axis=0, keepdims=True)\n",
    "        self.dgamma = np.sum(dout * self.x_norm, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to normalized input\n",
    "        dx_norm = dout * self.gamma\n",
    "        \n",
    "        # Gradient with respect to variance\n",
    "        dvar = np.sum(dx_norm * (self.input - self.batch_mean) * -0.5 * \n",
    "                     np.power(self.batch_var + self.epsilon, -1.5), axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to mean\n",
    "        dmean = np.sum(dx_norm * -1 / np.sqrt(self.batch_var + self.epsilon), axis=0, keepdims=True) + \\\n",
    "                dvar * np.mean(-2 * (self.input - self.batch_mean), axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to input\n",
    "        dx = dx_norm / np.sqrt(self.batch_var + self.epsilon) + \\\n",
    "             dvar * 2 * (self.input - self.batch_mean) / m + \\\n",
    "             dmean / m\n",
    "             \n",
    "        return dx\n",
    "\n",
    "    def update_params(self, learning_rate, momentum=0.9):\n",
    "        # Update with momentum\n",
    "        self.gamma_momentum = momentum * self.gamma_momentum - learning_rate * self.dgamma\n",
    "        self.beta_momentum = momentum * self.beta_momentum - learning_rate * self.dbeta\n",
    "        \n",
    "        self.gamma += self.gamma_momentum\n",
    "        self.beta += self.beta_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss_fn):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_fn\n",
    "        return\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, InvertedDropout):\n",
    "              x = layer.forward(x, training=training)\n",
    "            else:\n",
    "              x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, y, y_pred):\n",
    "        return self.loss_fn.forward(y, y_pred)\n",
    "\n",
    "    def backward(self, y_train):\n",
    "        grad = self.loss_fn.backward(y_train)\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            grad = np.clip(grad, -2, 2)\n",
    "            grad = self.layers[i].backward(grad)\n",
    "\n",
    "    def update(self, epoch):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.update_weights(decaying_lr(epoch))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X, training=False)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        correct_predictions = np.argmax(y_pred, axis=1) == np.argmax(y_test, axis=1)\n",
    "        accuracy = np.mean(correct_predictions)\n",
    "        return accuracy\n",
    "\n",
    "    def batch_train(self, X_train, y_train, epochs=100):\n",
    "        loss_hist = []\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X_train)\n",
    "            loss = self.loss(y_train, y_pred)\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "            loss_hist.append(loss)\n",
    "            self.backward(y_train)\n",
    "            self.update(epoch)\n",
    "        return loss_hist\n",
    "\n",
    "    def mini_batch_train(self, X_train, y_train, X_val, y_val, earlyStopping=False, epochs=100, batch=128):\n",
    "      loss_hist = []\n",
    "      avg_loss_hist = []\n",
    "      val_loss_hist = []\n",
    "      if earlyStopping:\n",
    "        cb = EarlyStopping()\n",
    "      for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train), batch):\n",
    "          X_train_batch = X_train[i: i+batch]\n",
    "          y_train_batch = y_train[i: i+batch]\n",
    "          y_pred = self.forward(X_train_batch)\n",
    "          loss = self.loss(y_train_batch, y_pred)\n",
    "          loss_hist.append(loss)\n",
    "          self.backward(y_train_batch)\n",
    "          self.update(epoch)\n",
    "        avg_loss = np.mean(loss_hist)\n",
    "        avg_loss_hist.append(avg_loss)\n",
    "        val_loss = self.predict_val(X_val, y_val)\n",
    "        val_loss_hist.append(val_loss)\n",
    "        if epoch % 5 == 0:\n",
    "              print(f\"Epoch: {epoch}, training loss: {avg_loss}, val loss: {val_loss}\")\n",
    "        criterea = cb.track(val_loss)\n",
    "        if not criterea:\n",
    "          print(f\"Early stopping on epoch {epoch}\")\n",
    "          return avg_loss_hist, val_loss_hist\n",
    "        loss_hist.clear()\n",
    "      return avg_loss_hist, val_loss_hist\n",
    "\n",
    "    def predict_val(self, X_val, y_val):\n",
    "      y_pred = self.forward(X_val)\n",
    "      loss = self.loss(y_val, y_pred)\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_hist, val_loss):\n",
    "    plt.plot(loss_hist, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss', linestyle='--')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Val Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(\n",
    "    [\n",
    "        Linear(X_train.shape[1], 600),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.5),\n",
    "        Linear(600, 300),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.5),\n",
    "        Linear(300,100),\n",
    "        ReLU(),\n",
    "        InvertedDropout(0.3),\n",
    "        # Linear(100,50),\n",
    "        # ReLU(),\n",
    "        InvertedDropout(0.3),\n",
    "        Linear(100, 10,weight_init=\"xavier\"),\n",
    "        Softmax(crossEntropy=True)\n",
    "    ],\n",
    "    CrossEntropy(softmax=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, training loss: 1.0066237202034014, val loss: 10.340173576858964\n",
      "Epoch: 5, training loss: 0.45977302268421044, val loss: 7.272696175673512\n",
      "Epoch: 10, training loss: 0.4019238038999724, val loss: 6.4519856953635655\n"
     ]
    }
   ],
   "source": [
    "X_train_sample = X_train\n",
    "y_train_sample = y_train\n",
    "loss_hist, val_loss_hist = model.mini_batch_train(X_train_sample, y_train_sample, X_val, y_val, earlyStopping=True, epochs=150)\n",
    "plot_loss(loss_hist, val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = model.evaluate(X_test, y_test)\n",
    "print(acc)\n",
    "\n",
    "y_pred = model.predict(X_test[:5])\n",
    "y = y_test[:5]\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "true_classes = np.argmax(y_test[:5], axis=1)\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "print(\"True classes:\", true_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
