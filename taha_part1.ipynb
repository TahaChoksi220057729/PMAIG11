{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "training_set = pd.read_csv('Dataset/fashion-mnist_train.csv')\n",
    "test_set = pd.read_csv('Dataset/fashion-mnist_test.csv')\n",
    "\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(df, target):\n",
    "    \n",
    "    # Extract feature columns\n",
    "    feature_columns = [col for col in df.columns if col != target]\n",
    "    \n",
    "    # Determine the type of the target column\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Convert features to float32\n",
    "    X = df[feature_columns].values.astype(np.float32)\n",
    "    \n",
    "    # Convert target to float32\n",
    "    y = df[target].values.astype(np.float32)\n",
    "    \n",
    "    # Handle target based on type\n",
    "    if target_type in (np.int64, np.int32):\n",
    "    # One hot encode for classification\n",
    "        y = pd.get_dummies(df[target]).values.astype(np.float32)\n",
    "    else:\n",
    "    # For regression, just convert to float32\n",
    "        y = df[target].values.astype(np.float32)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# X is pixels, y is labels\n",
    "\n",
    "X_train, y_train = data_prep(training_set, 'label')\n",
    "X_test, y_test = data_prep(test_set, 'label')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualising one image\n",
    "\n",
    "# choosing an image\n",
    "image_array = X_train[69]\n",
    "label = y_train[69]\n",
    "\n",
    "# reshape 784 to 28x28\n",
    "image_array = image_array.reshape(28, 28)\n",
    "\n",
    "# plotting the image\n",
    "plt.imshow(image_array, cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling the pixel values down to values between 0 - 1 for efficiency of neural network\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.name = 'tanh'\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "\n",
    "class LeakyReLU:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "        self.name = 'leaky_relu'\n",
    "\n",
    "    def forward(self, x):\n",
    "        return np.where(x > 0, x, self.alpha * x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1, self.alpha)\n",
    "    \n",
    "class Softmax:\n",
    "    def __init__(self, crossEntropy=False):\n",
    "        self.x_copy = None\n",
    "        self.cross_entropy = crossEntropy\n",
    "        self.output = None  # Store output for backward pass\n",
    "\n",
    "    def softmax(self, x):\n",
    "        stable_exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        y_prob = stable_exp_x / np.sum(stable_exp_x, axis=1, keepdims=True)\n",
    "        return y_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x_copy = x\n",
    "        self.output = self.softmax(x)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        batch_size = self.output.shape[0]\n",
    "        if self.cross_entropy:\n",
    "            # When used with cross-entropy loss, gradient simplifies\n",
    "            return dout\n",
    "        else:\n",
    "            # For each sample in the batch\n",
    "            jacobian = np.zeros_like(dout)\n",
    "            for i in range(batch_size):\n",
    "                S = self.output[i].reshape(-1, 1)\n",
    "                J = np.diagflat(S) - np.dot(S, S.T)\n",
    "                jacobian[i] = np.dot(J, dout[i])\n",
    "            return jacobian\n",
    "    \n",
    "class Linear:\n",
    "    def __init__(self, input_num, output_num):\n",
    "\n",
    "        # For gradient descent\n",
    "        self.db = None\n",
    "        self.dw = None\n",
    "        \n",
    "        # Set random seed for reproducibility\n",
    "        self.rng = np.random.default_rng(42)\n",
    "        \n",
    "        # Initialize weights using Xavier initialization\n",
    "        xavier_scale = np.sqrt(2.0 / (input_num + output_num))\n",
    "        self.weights = self.rng.normal(0, xavier_scale, size=(input_num, output_num))\n",
    "        \n",
    "        # Initialize biases to zeros\n",
    "        self.bias = np.zeros((1, output_num))\n",
    "        \n",
    "        # Store input for backprop\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass remains the same\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # Calculate gradients\n",
    "        self.dw = np.dot(self.input.T, grad_output)\n",
    "        self.db = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Store gradients for optimizer\n",
    "        self.weight_grad = self.dw\n",
    "        self.bias_grad = self.db\n",
    "        \n",
    "        # Gradient for next layer\n",
    "        dx = np.dot(grad_output, self.weights.T)\n",
    "        return dx\n",
    "    \n",
    "    def update_params(self, learning_rate):\n",
    "    \n",
    "        if self.dw is not None and self.db is not None:\n",
    "            self.weights -= learning_rate * self.dw\n",
    "            self.bias -= learning_rate * self.db\n",
    "        else:\n",
    "            print(\"Error: gradients not computed yet\")\n",
    "\n",
    "class CrossEntropyLoss:\n",
    "\n",
    "    def __init__(self, softmax=False):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "        self.softmax = softmax\n",
    "        self.batch_size = None\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.batch_size = y_pred.shape[0]\n",
    "\n",
    "        self.y_pred = np.clip(y_pred, 1e-15, 1-1e-15)\n",
    "        self.y_true = y_true\n",
    "\n",
    "        loss = -np.sum(y_true * np.log(self.y_pred)) / self.batch_size\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        return (self.y_pred - self.y_true) / self.batch_size\n",
    "\n",
    "class InvertedDropout:\n",
    "    def __init__(self, rate):\n",
    "        self.dropout_rate = rate\n",
    "        self.mask = None\n",
    "        self.rng = np.random.default_rng(42)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            init_mask = self.rng.random(x.shape)\n",
    "            self.mask = (init_mask > self.dropout_rate).astype(np.float32)\n",
    "            return x * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.9):\n",
    "        self.num_features = num_features\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "        self.running_mean = np.zeros((1, num_features))\n",
    "        self.running_var = np.ones((1, num_features))\n",
    "        self.training = True\n",
    "        # Store input for backward pass\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store input for backward pass\n",
    "        self.input = x\n",
    "        \n",
    "        if self.training:\n",
    "            batch_mean = np.mean(x, axis=0, keepdims=True)\n",
    "            batch_var = np.var(x, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update running statistics\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * batch_var\n",
    "            \n",
    "            # Normalize\n",
    "            x_norm = (x - batch_mean) / np.sqrt(batch_var + self.epsilon)\n",
    "            \n",
    "            # Store values for backward pass\n",
    "            self.batch_mean = batch_mean\n",
    "            self.batch_var = batch_var\n",
    "            self.x_norm = x_norm\n",
    "            \n",
    "        else:\n",
    "            # Use running statistics for inference\n",
    "            x_norm = (x - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_norm + self.beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # Get batch size\n",
    "        m = dout.shape[0]\n",
    "        \n",
    "        # Gradients for beta and gamma\n",
    "        self.dbeta = np.sum(dout, axis=0, keepdims=True)\n",
    "        self.dgamma = np.sum(dout * self.x_norm, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to normalized input\n",
    "        dx_norm = dout * self.gamma\n",
    "        \n",
    "        # Gradient with respect to variance\n",
    "        dvar = np.sum(dx_norm * (self.input - self.batch_mean) * -0.5 * \n",
    "                     np.power(self.batch_var + self.epsilon, -1.5), axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to mean\n",
    "        dmean = np.sum(dx_norm * -1 / np.sqrt(self.batch_var + self.epsilon), axis=0, keepdims=True) + \\\n",
    "                dvar * np.mean(-2 * (self.input - self.batch_mean), axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient with respect to input\n",
    "        dx = dx_norm / np.sqrt(self.batch_var + self.epsilon) + \\\n",
    "             dvar * 2 * (self.input - self.batch_mean) / m + \\\n",
    "             dmean / m\n",
    "             \n",
    "        return dx\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        self.gamma -= learning_rate * self.dgamma\n",
    "        self.beta -= learning_rate * self.dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss_function):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = loss_function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activation = x\n",
    "        for layer in self.layers:\n",
    "            activation = layer.forward(activation)\n",
    "        return activation\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "    \n",
    "    def update_params(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update_params'):\n",
    "                layer.update_params(self.learning_rate)\n",
    "\n",
    "    def train_mode(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'training'):\n",
    "                layer.training = True\n",
    "\n",
    "    def eval_mode(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'training'):\n",
    "                layer.training = False\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=100, batch_size=32, learning_rate=0.001):\n",
    "        self.train_mode()\n",
    "        self.learning_rate = learning_rate\n",
    "        n_samples = len(X_train)\n",
    "        losses = []\n",
    "        \n",
    "        # Create indices array for shuffling\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data at start of each epoch\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            total_loss = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            # Mini-batch training\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_X = X_shuffled[i:i + batch_size]\n",
    "                batch_y = y_shuffled[i:i + batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward(batch_X)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.loss_fn.forward(predictions, batch_y)\n",
    "                total_loss += loss\n",
    "                n_batches += 1\n",
    "                \n",
    "                # Backward pass\n",
    "                grad = self.loss_fn.backward()\n",
    "                self.backward(grad)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_params()\n",
    "            \n",
    "            # Calculate average loss per epoch\n",
    "            avg_loss = total_loss / n_batches\n",
    "            losses.append(avg_loss)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        self.eval_mode()\n",
    "        predictions = self.predict(X_test)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(y_test, axis=1)\n",
    "        accuracy = np.mean(predicted_classes == true_classes)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_hist):\n",
    "    plt.plot(loss_hist)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200, Loss: 1873546.2995\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    [\n",
    "        Linear(X_train.shape[1], 300),\n",
    "        BatchNorm(300),\n",
    "        LeakyReLU(),\n",
    "        InvertedDropout(0.2),\n",
    "        Linear(300, 10),       \n",
    "        BatchNorm(10),\n",
    "        Softmax()\n",
    "    ],\n",
    "    CrossEntropyLoss()\n",
    ")\n",
    "\n",
    "# Train the model with the updated learning rate\n",
    "loss_hist = model.train(X_train, y_train, epochs=100, batch_size=32, learning_rate=0.001)\n",
    "plot_loss(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "y_pred = model.predict(X_test[:100])\n",
    "y = y_test[:300]\n",
    "predicted_classes = np.argmax(y_pred, axis=1)\n",
    "true_classes = np.argmax(y_test[:100], axis=1)\n",
    "print(\"Predicted classes:\", predicted_classes)\n",
    "print(\"True classes:\", true_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
